{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LannetTech_Internshala_Task.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63SMd-dQtSDc"
      },
      "source": [
        "### __1) Write a python function which take a dataframe as input and deals with the issue of outliers in all the continuous variables.__\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7n112EIzqMW"
      },
      "source": [
        "__Solution :__ The function _OutlrTrtmnt(cont)_ will find the continuous values and then, remove the outliers through Inter Quantile Range technique. Although, it is a very basic technique but, robust to work as standalone pack with different kinds of datasets.\n",
        "\n",
        "Impovisations: Adding 2-3 more outlier techniques would make the function a good auto-outlier detection(likely) library\\pack."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD95AOjGtMvU"
      },
      "source": [
        "###Importing Packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "#drive.mount('/content/gdrive/')\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\"\"\"Assumptions for the dataset:\n",
        "   1. Data is partially preprocessed(apart from outliers treatment)\n",
        "   2. There are no missing or NaN values\"\"\" \n",
        "\n",
        "  \n",
        "### Loading Data\n",
        "Data = pd.read_csv(input(\"Please input Dataset link\"))         # Runtime input is to be provided\n",
        "df = pd.DataFrame(Data)\n",
        "#df.head()                                                      # For vieweing the input DataFrame\n",
        "\n",
        "df = df.replace(np.NaN, df.mean())                             # A basic preprocessing step of removing NaN values.\n",
        "\n",
        "### Finding continuous data columns in the Dataset\n",
        "iscont = {}                                                    # Dictionary will store the names of coninuous columns\n",
        "for col in df.columns:\n",
        "    iscont[col] = df[col].nunique() > (0.05/df[col].count())   # If uniques in the data are more than 5% of rows then, the column is considered as continuous\n",
        "#The boolean output will be stored inside 'iscont' dict to compare and identify continuuos columns. Where, True -> more than 5% uniques and False -> less than 5%uniques.\n",
        "#To cover all kinds of data, 5% has been choosen as threshold.\n",
        "\n",
        "cont = pd.DataFrame()                                          # Initializing continuous columns dataframe.\n",
        "count = 0                                                      # initializing count for message, if there are continuous data columns in the dataset\n",
        "for col in df.columns:    \n",
        "  if ((iscont[col]==True) & (df[col].dtype == float)):         # If uniques are greater than 5%(which is True) and Column type is float then, the column in considered as contiuous \n",
        "    cont[col] = df[col]                                        # 'cont' is the dataframe with all the continuous data columns. \n",
        "    count +=1\n",
        "if(count<1):\n",
        "    print('There are no continuous data columns in the given dataset')\n",
        "\n",
        "#cont.head()                                                    # For viewing the seperated Continuous DataFrame\n",
        "\n",
        "### Defining a function for removing outliers using Quantile Ranges:\n",
        "def OutlrTrtmnt(cont):\n",
        "    Q1 = cont.quantile(0.10)                                   # Calculating Q1 value using quantile(0.10)\n",
        "    Q3 = cont.quantile(0.90)                                   # Calculating Q3 value using quantile value (0.90)\n",
        "    Q = Q3-Q1                                                  # Calculating Inter Quartile Range\n",
        "    NoOutlr = cont[~((cont<(Q1-Q))|(cont>(Q3+Q))).any(axis=1)] # Checking for outliers and droping them off\n",
        "    return NoOutlr                                             # returning the created variable\n",
        "\n",
        "### Scaling continuous column dataset so that, all the columns are in same scale while removing outliers.\n",
        "scaler = StandardScaler().fit(cont) \n",
        "scaledData = scaler.transform(cont)\n",
        "\n",
        "cont = OutlrTrtmnt(cont)\n",
        "print(len(df)-len(cont), \"Outliers were found and removed through Quantile Range\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA7VAgmOzPlq"
      },
      "source": [
        "### __2) Write a function in python that inputs a dataframe and identify which columns have date in them. Using these date columns make new columns which are difference between these columns taking 2 at a time. (for instance if there is date1, date2, date3 columns, output should be like date1-date2, date2-date3, date1-date3 )__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRrI6A641AI5"
      },
      "source": [
        "__Solution :__ The random date dataset is created and input to the function _diffofdates(a,b)_ for identifying the date columns. Once found, all the dates columns undergo differences inbetween with two at a time. \n",
        "\n",
        "The function is built efficient to any kind of date dataset. And the output of the function is a dataframe with columns names as the date columns combinations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6DEz5XazUx7"
      },
      "source": [
        "#Importing Packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "#drive.mount('/content/gdrive/')\n",
        "from datetime import date\n",
        "from itertools import combinations\n",
        "\n",
        "### Generating random date dataframe as an input\n",
        "np.random.seed(0)\n",
        "rnge1 = pd.date_range('1980-01-01','2019-12-31', periods=50)\n",
        "rnge2 = pd.date_range('1956-01-01', '2009-12-31',periods=50)\n",
        "rnge3 = pd.date_range('1856-01-01', '2020-07-24',periods=50)\n",
        "rnge4 = pd.date_range('2000-01-01', '2009-12-31',periods=50)\n",
        "rnge5 = pd.date_range('1992-04-17', '1994-07-15',periods=50)\n",
        "df = pd.DataFrame({ 'Date1': rnge1,'Date2': rnge2,'Date3': rnge3,'Date4': rnge4,'Date5': rnge5,'Value1': np.random.randn(len(rng1)), 'Value2': np.random.randn(len(rng2))}) \n",
        "\n",
        "\n",
        "#df.head()                                                      # For vieweing the input DataFrame\n",
        "#Note: Here, the numeric columns: Value1 and Value2 are created only to show that the function can seperate the date from any kind of given dataset.\n",
        "\n",
        "\n",
        "### Finding and converting 'date' columns to data type -> 'datetime64[ns]'\n",
        "for col in df.columns:\n",
        "  if df[col].dtype == 'object':                                # First step of filtering using default 'object' data type \n",
        "    try:\n",
        "      df[col] = pd.to_datetime(df[col])                        # With try-except, the data type of date columns have been changed to 'datetime64[ns]'\n",
        "    except ValueError: \n",
        "      pass\n",
        "\n",
        "\n",
        "\n",
        "### Finding and seperating date data columns from the dataset\n",
        "od=pd.DataFrame()                                              # Initializing dataframe for date columns\n",
        "for col in df.columns:\n",
        "  if df[col].dtype == '<M8[ns]':                               # if the dtype of column is '<M8[ns]'(which datetime dtype) then, the column is considered to be date column\n",
        "    od[col] = [d.date() for d in df[col]]                      \n",
        "    od[col] = df[col].dt.strftime('%Y/%m/%d')                  # Tuning various date formats in the dataset into single format -> '%Y/%m/%d'\n",
        "    od[col] = pd.to_datetime(od[col])                          # In the date columns only dataframe, the dtype is defaulted to object. So, we change it back to 'datetime64[ns]'. because, only then, we can perform date arthimetic operations.\n",
        "\n",
        "\n",
        "#od.head()                                                      # Display of identified date columns\n",
        "\n",
        "lst = list(combinations(od.columns,2))                         # Created a list for binding all the columns 2 at a time.\n",
        "#lst                                                           # Display of combinations in the list\n",
        "\"\"\" The list helps in creating the arthimetic combinations among the columns. Ex: if a,b,c are the columns.\n",
        "    lst gives the combination such as, (a,b), (b,c), (c,a) and these are passed to function\"\"\"\n",
        "lst\n",
        "\n",
        "### Defining a function for performing arthematic operations on the date columns to find days\n",
        "dd=pd.DataFrame()\n",
        "def diffofdates(a,b):                                          # Functions with two parameters: a-> first date column and b -> second date column for first iteration as seen in above example.\n",
        "  for i in lst:\n",
        "    dd[str(i)]= (od[a] - od[b]).dt.days                        # One-by-one all the combinations will undergo difference operation\n",
        "\n",
        "### Run the function\n",
        "for a,b in lst:                                                # Looping over the combination to pass values to the function\n",
        "  diffofdates(a,b)                                             # Finally triggering the function\n",
        "\n",
        "\n",
        "### The output finds the date columns and gives the difference of the dates in dataset\n",
        "dd.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "im5EaSfhGqOq"
      },
      "source": [
        "## __3) Write a function in python that take dataframe as input and drop columns having Pearson correlation more than 0.85__\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbIFY3-HNPvz"
      },
      "source": [
        "__Solution:__ Once given the dataset, the function evaluates correlation among the columns. Considering the threshold for 0.85, all the columns above it are dropped. The output of the function is dataframe with columns having pearson correlation less than 0.85."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xetf5TYC9wj"
      },
      "source": [
        "#Importing Packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "\"\"\"Assumptions for the dataset:\n",
        "   1. Data is partially preprocessed(apart from outliers treatment)\n",
        "   2. There are no missing or NaN values\"\"\"\n",
        "\n",
        "### Loading Data\n",
        "Data = pd.read_csv(input(\"Please input Dataset link\"))                                # Runtime input is to be provided\n",
        "df = pd.DataFrame(Data)\n",
        "\n",
        "df = df.replace(np.NaN, df.mean())                                                    # A basic preprocessing step of removing NaN values.\n",
        "\n",
        "#df.head()\n",
        " \n",
        "corr_matrix = df.corr().abs()                                                         # Creating correlation matrix\n",
        "#corr_matrix                             \n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))   # Filtering the upper triangle of correlation matrix using numpys\n",
        "\n",
        "to_drop = [column for column in upper.columns if any(upper[column] > 0.85)]           # Finding index of feature column which have pearson correlation > 0.85\n",
        "''' Every column is checked if its correlation is more than 85 or not. If yes, then it will dropped'''\n",
        "df.drop(df[to_drop], axis=1)                                                          # Dropping the features and displaying the modified dataset "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}